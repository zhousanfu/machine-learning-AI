{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhousanfu/machine-learning-demo/blob/master/LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbEDIhXr5QYP"
      },
      "outputs": [],
      "source": [
        "!pip install langchain faiss-cpu\n",
        "!pip install transformers sentence_transformers sentencepiece cpm_kernels\n",
        "!pip install google-search-results -i pypi.douban.com/simple --trusted-host pypi.douban.com"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatglm"
      ],
      "metadata": {
        "id": "7uiRt-JmBtjz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbBCts3O3mX8"
      },
      "outputs": [],
      "source": [
        "from typing import Any, List, Mapping, Optional\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "class chatGLM():\n",
        "    def __init__(self, model_name, quantization_bit=4) -> None:\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "        if torch.cuda.is_available():\n",
        "            self.model = AutoModel.from_pretrained(model_name, trust_remote_code=True).half().cuda().eval()\n",
        "        else:\n",
        "            self.model = model = AutoModel.from_pretrained(model_name, trust_remote_code=True).float()\n",
        "        self.model = self.model.quantize(quantization_bit)\n",
        "\n",
        "    def __call__(self, prompt, history=None) -> Any:\n",
        "        response, history = self.model.chat(self.tokenizer , prompt, history=history) # è¿™é‡Œæ¼”ç¤ºæœªä½¿ç”¨æµå¼æ¥å£. stream_chat()\n",
        "        return response, history\n",
        "\n",
        "llm = chatGLM(model_name=\"THUDM/chatglm-6B-int4\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response, history = llm(prompt=\"ä½ å¥½\", history=[])\n",
        "print(\"response: %s\"%response)\n",
        "response, history = llm(prompt=\"æˆ‘æœ€è¿‘æœ‰ç‚¹å¤±çœ æ€ä¹ˆåŠ?\", history=history)\n",
        "print(\"response: %s\"%response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8MrQIdOBhtc",
        "outputId": "1a5783c1-ff3d-4cd5-cfdc-a253c3ef54d0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.THUDM.chatglm-6B-int4.6c5205c47d0d2f7ea2e44715d279e537cae0911f.modeling_chatglm:The dtype of attention mask (torch.int64) is not bool\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response: ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\n",
            "response: å¤±çœ çš„è¯ï¼Œå»ºè®®å°è¯•ä¸‹è¿°æ–¹æ³•è¿›è¡Œæ”¹å–„ï¼š\n",
            "1. ç¡å‰æ”¾æ¾ï¼šåœ¨ç¡å‰åŠå°æ—¶å°½é‡é¿å…è¿›è¡Œåˆºæ¿€æ€§çš„æ´»åŠ¨ï¼Œæ”¾æ¾èº«å¿ƒï¼Œä¾‹å¦‚æ³¡ä¸ªçƒ­æ°´æ¾¡ã€å¬è½»æŸ”çš„éŸ³ä¹ç­‰ã€‚\n",
            "2. å»ºç«‹è§„å¾‹çš„ç¡çœ æ—¶é—´ï¼šä¿æŒæ¯å¤©å›ºå®šçš„ç¡çœ æ—¶é—´å’Œèµ·åºŠæ—¶é—´ï¼Œå¸®åŠ©èº«ä½“å»ºç«‹ä¸€ä¸ªå¥åº·çš„ç¡çœ èŠ‚å¾‹ã€‚\n",
            "3. æ”¹å–„ç¡å‰ç¯å¢ƒï¼šä¾‹å¦‚è°ƒæš—ç¯å…‰ã€ä¿æŒå®‰é™ã€å‡å°‘å™ªéŸ³ç­‰ï¼Œæˆ–è€…å°è¯•ä½¿ç”¨æ”¾æ¾æ€§è´¨çš„å¸ƒç½®ï¼Œå¦‚èˆ’é€‚ã€æŸ”è½¯çš„åºŠä¸Šç”¨å“ç­‰ã€‚\n",
            "4. é¿å…å’–å•¡å› å’Œé…’ç²¾ï¼šè¿™äº›ç‰©è´¨ä¼šå½±å“ç¡çœ ï¼Œå°½é‡é¿å…æ‘„å…¥ã€‚\n",
            "5. é”»ç‚¼èº«ä½“ï¼šé€‚åº¦çš„è¿åŠ¨å¯ä»¥æé«˜èº«ä½“çš„ä»£è°¢æ°´å¹³ï¼Œå¸®åŠ©å…¥ç¡ã€‚\n",
            "å¦‚æœä¸Šè¿°æ–¹æ³•æ— æ³•æ”¹å–„å¤±çœ ï¼Œå»ºè®®å’¨è¯¢åŒ»ç”Ÿæˆ–ä¸“ä¸šçš„åŒ»ç–—æœºæ„ï¼Œå¯»æ±‚æ›´è¿›ä¸€æ­¥çš„å¸®åŠ©ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langchain\n",
        "\n",
        "LangChainæ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ï¼Œæ—¨åœ¨å¸®åŠ©å¼€å‘äººå‘˜ä½¿ç”¨è¯­è¨€æ¨¡å‹æ„å»ºç«¯åˆ°ç«¯çš„åº”ç”¨ç¨‹åºã€‚å®ƒæä¾›äº†ä¸€å¥—å·¥å…·ã€ç»„ä»¶å’Œæ¥å£ï¼Œå¯ç®€åŒ–åˆ›å»ºç”±å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) å’ŒèŠå¤©æ¨¡å‹æä¾›æ”¯æŒçš„åº”ç”¨ç¨‹åºçš„è¿‡ç¨‹ã€‚LangChain å¯ä»¥è½»æ¾ç®¡ç†ä¸è¯­è¨€æ¨¡å‹çš„äº¤äº’ï¼Œå°†å¤šä¸ªç»„ä»¶é“¾æ¥åœ¨ä¸€èµ·ï¼Œå¹¶é›†æˆé¢å¤–çš„èµ„æº"
      ],
      "metadata": {
        "id": "hMURU65yCiBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # å®˜æ–¹llmä½¿ç”¨OPENAI æ¥å£\n",
        "# from langchain.llms import OpenAI\n",
        "# llm = OpenAI(model_name=\"text-davinci-003\")\n",
        "# prompt = \"ä½ å¥½\"\n",
        "# response = llm(prompt)"
      ],
      "metadata": {
        "id": "RS4PLFqgSXtz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### google-search"
      ],
      "metadata": {
        "id": "dS7ZeLoMv3dY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.agents import load_tools\n",
        "# import os\n",
        "\n",
        "\n",
        "# os.environ[\"SERPAPI_API_KEY\"] = 'ä½ çš„api key'\n",
        "\n",
        "# tools = load_tools([\"serpapi\", \"python_repl\", \"llm-math\"], llm=llm)\n",
        "# agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "# agent.run(\"å½“å‰é‡‘ä»·æ˜¯å¤šå°‘ä¸€å…‹ï¼Ÿ\")"
      ],
      "metadata": {
        "id": "eWOG03owv-Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prompt æç¤º\n",
        "å¡«å…¥å†…å®¹æ¥å¼•å¯¼å¤§æ¨¡å‹è¾“å‡º"
      ],
      "metadata": {
        "id": "7SE976V_CuP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "template = \"\"\"ä»€ä¹ˆæ˜¯{query},è¿˜æœ‰å¦‚ä½•çœŸæ­£åšåˆ°å¹¶ç»†è¯´å®ç°æ­¥éª¤\"\"\"\n",
        "prompt_tem = PromptTemplate(input_variables=[\"query\"], template=template)\n",
        "prompt = prompt_tem.format(query='é˜¶çº§è·³è·ƒ')\n",
        "\n",
        "response, history = llm(prompt=prompt, history=[])\n",
        "print(prompt, '-->', response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPt2QqzyCtsB",
        "outputId": "d25150b4-1db3-4645-9b25-54fe71dd61b8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä»€ä¹ˆæ˜¯é˜¶çº§è·³è·ƒ,è¿˜æœ‰å¦‚ä½•çœŸæ­£åšåˆ°å¹¶ç»†è¯´å®ç°æ­¥éª¤ --> é˜¶çº§è·³è·ƒæ˜¯æŒ‡åœ¨æŸä¸ªæ¸¸æˆä¸­ï¼Œç©å®¶é€šè¿‡ç§¯ç´¯ç‰¹å®šçš„é“å…·å’ŒæŠ€èƒ½ï¼Œä»æ™®é€šç©å®¶å˜æˆé«˜çº§ç©å®¶çš„è¿‡ç¨‹ã€‚è¿™ç§è¿‡ç¨‹éœ€è¦ç©å®¶åœ¨æ¸¸æˆä¸­ä¸æ–­æŒ‘æˆ˜è‡ªå·±ï¼Œæé«˜è‡ªå·±çš„æŠ€èƒ½å’Œé“å…·çš„åˆ©ç”¨æ•ˆç‡ï¼Œä»è€Œè·å¾—æ›´å¤šçš„æ¸¸æˆå¥–åŠ±å’Œæˆå°±ã€‚\n",
            "\n",
            "è¦å®ç°é˜¶çº§è·³è·ƒï¼Œéœ€è¦ç©å®¶æŒæ¡ä»¥ä¸‹æŠ€èƒ½å’Œç­–ç•¥ï¼š\n",
            "\n",
            "1. é“å…·çš„ä½¿ç”¨ï¼šç©å®¶éœ€è¦å­¦ä¼šå¦‚ä½•æœ‰æ•ˆåœ°ä½¿ç”¨æ¸¸æˆä¸­çš„å„ç§é“å…·ï¼Œå¦‚æŠ€èƒ½ã€è£…å¤‡ã€é‡‘å¸ç­‰ï¼Œä»¥æé«˜è‡ªå·±çš„æˆ˜æ–—åŠ›å’Œå±æ€§ã€‚\n",
            "\n",
            "2. æŠ€èƒ½çš„å­¦ä¹ ï¼šç©å®¶éœ€è¦å­¦ä¹ æ¸¸æˆä¸­çš„å„ç§æŠ€èƒ½ï¼Œå¦‚æˆ˜æ–—æŠ€èƒ½ã€æ“ä½œæŠ€èƒ½ã€ç­–ç•¥æŠ€èƒ½ç­‰ï¼Œä»¥ä¾¿åœ¨æ¸¸æˆä¸­æ›´å¥½åœ°å‘æŒ¥è‡ªå·±çš„èƒ½åŠ›ã€‚\n",
            "\n",
            "3. æŒ‘æˆ˜å’Œå‡çº§ï¼šç©å®¶éœ€è¦ä¸æ–­æŒ‘æˆ˜æ¸¸æˆä¸­çš„å„ç§éš¾åº¦æ¨¡å¼ï¼Œå¦‚æ™®é€šã€ä¸“å®¶ã€å¤§å¸ˆç­‰ï¼Œä»¥æé«˜è‡ªå·±çš„æŠ€èƒ½å’Œé“å…·çš„ç­‰çº§ã€‚\n",
            "\n",
            "4. é‡‘å¸çš„ä½¿ç”¨ï¼šç©å®¶éœ€è¦å­¦ä¼šå¦‚ä½•æœ‰æ•ˆåœ°ä½¿ç”¨æ¸¸æˆä¸­çš„é‡‘å¸ï¼Œä»¥ä¾¿è´­ä¹°æ›´å¤šçš„é“å…·å’ŒæŠ€èƒ½ï¼Œæé«˜è‡ªå·±çš„æˆ˜æ–—åŠ›å’Œå±æ€§ã€‚\n",
            "\n",
            "ä»¥ä¸‹æ˜¯å®ç°é˜¶çº§è·³è·ƒçš„å…·ä½“æ­¥éª¤ï¼š\n",
            "\n",
            "1. å­¦ä¹ æŠ€èƒ½ï¼šç©å®¶éœ€è¦å­¦ä¹ æ¸¸æˆä¸­çš„å„ç§æŠ€èƒ½ï¼Œå¹¶äº†è§£æ¯ä¸ªæŠ€èƒ½çš„ä½œç”¨å’Œä¼˜ç‚¹ï¼Œä»¥ä¾¿æ›´å¥½åœ°åˆ©ç”¨è¿™äº›æŠ€èƒ½æ¥æé«˜è‡ªå·±çš„æˆ˜æ–—åŠ›å’Œå±æ€§ã€‚\n",
            "\n",
            "2. æŒ‘æˆ˜éš¾åº¦æ¨¡å¼ï¼šç©å®¶éœ€è¦ä¸æ–­æŒ‘æˆ˜æ¸¸æˆä¸­çš„å„ç§éš¾åº¦æ¨¡å¼ï¼Œå¦‚æ™®é€šã€ä¸“å®¶ã€å¤§å¸ˆç­‰ï¼Œä»¥æé«˜è‡ªå·±çš„æŠ€èƒ½å’Œé“å…·çš„ç­‰çº§ã€‚\n",
            "\n",
            "3. è´­ä¹°é“å…·å’ŒæŠ€èƒ½ï¼šç©å®¶éœ€è¦è´­ä¹°æ¸¸æˆä¸­çš„å„ç§é“å…·å’ŒæŠ€èƒ½ï¼Œå¹¶äº†è§£å®ƒä»¬çš„ä½œç”¨å’Œä¼˜ç‚¹ï¼Œä»¥ä¾¿æ›´å¥½åœ°åˆ©ç”¨è¿™äº›é“å…·å’ŒæŠ€èƒ½æ¥æé«˜è‡ªå·±çš„æˆ˜æ–—åŠ›å’Œå±æ€§ã€‚\n",
            "\n",
            "4. ä½¿ç”¨é‡‘å¸ï¼šç©å®¶éœ€è¦å­¦ä¼šå¦‚ä½•æœ‰æ•ˆåœ°ä½¿ç”¨æ¸¸æˆä¸­çš„é‡‘å¸ï¼Œä»¥ä¾¿è´­ä¹°æ›´å¤šçš„é“å…·å’ŒæŠ€èƒ½ï¼Œæé«˜è‡ªå·±çš„æˆ˜æ–—åŠ›å’Œå±æ€§ã€‚\n",
            "\n",
            "5. æŒç»­å­¦ä¹ å’Œæé«˜ï¼šç©å®¶éœ€è¦ä¸æ–­å­¦ä¹ å’Œæé«˜è‡ªå·±çš„æŠ€èƒ½å’Œé“å…·çš„ç­‰çº§ï¼Œä»¥ä¾¿åœ¨æ¸¸æˆä¸­æ›´å¥½åœ°å‘æŒ¥è‡ªå·±çš„èƒ½åŠ›ï¼Œå®ç°é˜¶çº§è·³è·ƒã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_template = \"ä½ æ˜¯ä¸€ä¸ªæŠŠ{input_language}ç¿»è¯‘æˆ{output_language}çš„åŠ©æ‰‹\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "human_template = \"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "messages = chat_prompt.format_prompt(input_language=\"è‹±è¯­\", output_language=\"æ±‰è¯­\", text=\"I love programming.\")\n",
        "\n",
        "messages.to_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_m3-jfGB7qs",
        "outputId": "d22752fb-fa61-415a-8f26-7eee0f7da52a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªæŠŠè‹±è¯­ç¿»è¯‘æˆæ±‰è¯­çš„åŠ©æ‰‹', additional_kwargs={}),\n",
              " HumanMessage(content='I love programming.', additional_kwargs={}, example=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chains é“¾\n",
        "é“¾æ¥å¤šä¸ªç»„ä»¶å¤„ç†ä¸€ä¸ªç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡"
      ],
      "metadata": {
        "id": "4_MqGD66Di6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.chains import LLMChain\n",
        "# chain = LLMChain(llm=openAI(), prompt=promptTem)\n",
        "# print(chain.run(\"ä½ å¥½\"))\n",
        "\n",
        "from langchain.chains.base import Chain\n",
        "\n",
        "\n",
        "\n",
        "class DemoChain():\n",
        "    def __init__(self, llm, prompt, history) -> None:\n",
        "        self.llm = llm\n",
        "        self.prompt = prompt\n",
        "        self.history = history\n",
        "\n",
        "    def run(self, query, history, context=None) -> Any:\n",
        "        if context is not None:\n",
        "            prompt = self.prompt.format(query=query, context=context)\n",
        "        else:\n",
        "            prompt = self.prompt.format(query=query)\n",
        "\n",
        "        response, history = self.llm(prompt, history)\n",
        "        return response, history\n",
        "\n",
        "chain = DemoChain(llm=llm, prompt=prompt_tem, history=[])\n",
        "response, history = chain.run(query=\"é˜¶çº§è·³è·ƒ\", history=[])\n",
        "print(response, history)\n",
        "\n",
        "chain = DemoChain(llm=llm, prompt=prompt_tem, history=[])\n",
        "response, history = chain.run(query=\"é˜¶çº§è·³è·ƒ\", history=[])\n",
        "print(response, history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYEVjYq4DlrN",
        "outputId": "93831244-56e3-4c52-bb5d-d3b7f7bb4f78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "é˜¶çº§è·³è·ƒæ˜¯æŒ‡ä¸€ä¸ªäººæˆ–ç»„ç»‡é€šè¿‡ä¸æ–­åŠªåŠ›å’Œå­¦ä¹ ï¼Œä»ä¸€ä¸ªé˜¶çº§è·¨è¶Šåˆ°å¦ä¸€ä¸ªé˜¶çº§çš„è¿‡ç¨‹ã€‚è¿™ä¸ªè¿‡ç¨‹é€šå¸¸æ¶‰åŠåˆ°å¯¹ç¤¾ä¼šç»“æ„å’Œæ¸¸æˆè§„åˆ™çš„æ·±åˆ»ç†è§£ï¼Œä»¥åŠåœ¨é€‚å½“çš„æ—¶é—´å’Œåœºåˆä¸‹é‡‡å–é€‚å½“çš„è¡ŒåŠ¨ã€‚\n",
            "\n",
            "å®ç°é˜¶çº§è·³è·ƒçš„æ­¥éª¤å¦‚ä¸‹ï¼š\n",
            "\n",
            "1. å­¦ä¹ æ¸¸æˆè§„åˆ™å’Œå†å²ç»éªŒï¼šé˜¶çº§è·³è·ƒéœ€è¦å¯¹æ¸¸æˆè§„åˆ™å’Œå†å²ç»éªŒæœ‰æ·±å…¥çš„äº†è§£ã€‚ç©å®¶éœ€è¦å­¦ä¹ å¦‚ä½•åœ¨æ¸¸æˆä¸­è·å¾—ä¼˜åŠ¿ï¼Œå¦‚ä½•èµ¢å¾—æ¯”èµ›ï¼Œä»¥åŠå¦‚ä½•ä¸å¯¹æ‰‹ç«äº‰ã€‚\n",
            "\n",
            "2. å»ºç«‹å¼ºå¤§çš„å…³ç³»ç½‘ç»œï¼šå…³ç³»ç½‘ç»œå¯ä»¥å¸®åŠ©ç©å®¶è·å¾—æ¸¸æˆä¸­çš„ä¼˜åŠ¿ã€‚ç©å®¶éœ€è¦ä¸å…¶ä»–äººå»ºç«‹è”ç³»ï¼Œä»¥è·å¾—ä»–ä»¬çš„ä¿¡ä»»å’Œèµ„æºã€‚\n",
            "\n",
            "3. æé«˜è‡ªå·±çš„æŠ€èƒ½ï¼šç©å®¶éœ€è¦ä¸æ–­æé«˜è‡ªå·±çš„æŠ€èƒ½ï¼Œä»¥å¢åŠ åœ¨æ¸¸æˆä¸­çš„ä¼˜åŠ¿ã€‚è¿™å¯èƒ½åŒ…æ‹¬å­¦ä¹ æ–°æŠ€èƒ½ã€è·å¾—æ›´å¥½çš„å·¥ä½œç»éªŒæˆ–è·å¾—æ›´é«˜çš„å­¦å†ã€‚\n",
            "\n",
            "4. é€‰æ‹©é€‚å½“çš„æ—¶é—´å’Œåœºåˆï¼šé˜¶çº§è·³è·ƒéœ€è¦åœ¨é€‚å½“çš„æ—¶é—´å’Œåœºåˆä¸‹è¿›è¡Œã€‚ç©å®¶éœ€è¦é€‰æ‹©é€‚å½“çš„æ—¶é—´è¿›è¡Œé˜¶çº§è·³è·ƒï¼Œä¾‹å¦‚åœ¨èµ¢å¾—æ¯”èµ›ä¹‹å‰è¿›è¡Œé˜¶çº§è·³è·ƒï¼Œæˆ–è€…åœ¨æ¯”èµ›ä¸­è·å¾—ä¼˜åŠ¿ä¹‹åè¿›è¡Œé˜¶çº§è·³è·ƒã€‚\n",
            "\n",
            "5. å®æ–½é˜¶çº§è·³è·ƒï¼šç©å®¶éœ€è¦é‡‡å–é€‚å½“çš„è¡ŒåŠ¨æ¥å®æ–½é˜¶çº§è·³è·ƒã€‚è¿™å¯èƒ½åŒ…æ‹¬å»ºç«‹å¼ºå¤§çš„å…³ç³»ç½‘ç»œã€æé«˜è‡ªå·±çš„æŠ€èƒ½ã€é€‰æ‹©é€‚å½“çš„æ—¶é—´å’Œåœºåˆï¼Œä»¥åŠå®æ–½é˜¶çº§è·³è·ƒã€‚\n",
            "\n",
            "é˜¶çº§è·³è·ƒæ˜¯ä¸€ä¸ªéœ€è¦ä¸æ–­å­¦ä¹ å’Œæ”¹è¿›çš„è¿‡ç¨‹ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªéœ€è¦è€å¿ƒå’Œæ¯…åŠ›çš„è¿‡ç¨‹ã€‚åªæœ‰å¯¹äºé‚£äº›å…·æœ‰åšå®šçš„ä¿¡å¿µå’Œæ¯…åŠ›çš„äººæ¥è¯´ï¼Œé˜¶çº§è·³è·ƒæ‰èƒ½çœŸæ­£å®ç°ã€‚ [('ä»€ä¹ˆæ˜¯é˜¶çº§è·³è·ƒ,è¿˜æœ‰å¦‚ä½•çœŸæ­£åšåˆ°å¹¶ç»†è¯´å®ç°æ­¥éª¤', 'é˜¶çº§è·³è·ƒæ˜¯æŒ‡ä¸€ä¸ªäººæˆ–ç»„ç»‡é€šè¿‡ä¸æ–­åŠªåŠ›å’Œå­¦ä¹ ï¼Œä»ä¸€ä¸ªé˜¶çº§è·¨è¶Šåˆ°å¦ä¸€ä¸ªé˜¶çº§çš„è¿‡ç¨‹ã€‚è¿™ä¸ªè¿‡ç¨‹é€šå¸¸æ¶‰åŠåˆ°å¯¹ç¤¾ä¼šç»“æ„å’Œæ¸¸æˆè§„åˆ™çš„æ·±åˆ»ç†è§£ï¼Œä»¥åŠåœ¨é€‚å½“çš„æ—¶é—´å’Œåœºåˆä¸‹é‡‡å–é€‚å½“çš„è¡ŒåŠ¨ã€‚\\n\\nå®ç°é˜¶çº§è·³è·ƒçš„æ­¥éª¤å¦‚ä¸‹ï¼š\\n\\n1. å­¦ä¹ æ¸¸æˆè§„åˆ™å’Œå†å²ç»éªŒï¼šé˜¶çº§è·³è·ƒéœ€è¦å¯¹æ¸¸æˆè§„åˆ™å’Œå†å²ç»éªŒæœ‰æ·±å…¥çš„äº†è§£ã€‚ç©å®¶éœ€è¦å­¦ä¹ å¦‚ä½•åœ¨æ¸¸æˆä¸­è·å¾—ä¼˜åŠ¿ï¼Œå¦‚ä½•èµ¢å¾—æ¯”èµ›ï¼Œä»¥åŠå¦‚ä½•ä¸å¯¹æ‰‹ç«äº‰ã€‚\\n\\n2. å»ºç«‹å¼ºå¤§çš„å…³ç³»ç½‘ç»œï¼šå…³ç³»ç½‘ç»œå¯ä»¥å¸®åŠ©ç©å®¶è·å¾—æ¸¸æˆä¸­çš„ä¼˜åŠ¿ã€‚ç©å®¶éœ€è¦ä¸å…¶ä»–äººå»ºç«‹è”ç³»ï¼Œä»¥è·å¾—ä»–ä»¬çš„ä¿¡ä»»å’Œèµ„æºã€‚\\n\\n3. æé«˜è‡ªå·±çš„æŠ€èƒ½ï¼šç©å®¶éœ€è¦ä¸æ–­æé«˜è‡ªå·±çš„æŠ€èƒ½ï¼Œä»¥å¢åŠ åœ¨æ¸¸æˆä¸­çš„ä¼˜åŠ¿ã€‚è¿™å¯èƒ½åŒ…æ‹¬å­¦ä¹ æ–°æŠ€èƒ½ã€è·å¾—æ›´å¥½çš„å·¥ä½œç»éªŒæˆ–è·å¾—æ›´é«˜çš„å­¦å†ã€‚\\n\\n4. é€‰æ‹©é€‚å½“çš„æ—¶é—´å’Œåœºåˆï¼šé˜¶çº§è·³è·ƒéœ€è¦åœ¨é€‚å½“çš„æ—¶é—´å’Œåœºåˆä¸‹è¿›è¡Œã€‚ç©å®¶éœ€è¦é€‰æ‹©é€‚å½“çš„æ—¶é—´è¿›è¡Œé˜¶çº§è·³è·ƒï¼Œä¾‹å¦‚åœ¨èµ¢å¾—æ¯”èµ›ä¹‹å‰è¿›è¡Œé˜¶çº§è·³è·ƒï¼Œæˆ–è€…åœ¨æ¯”èµ›ä¸­è·å¾—ä¼˜åŠ¿ä¹‹åè¿›è¡Œé˜¶çº§è·³è·ƒã€‚\\n\\n5. å®æ–½é˜¶çº§è·³è·ƒï¼šç©å®¶éœ€è¦é‡‡å–é€‚å½“çš„è¡ŒåŠ¨æ¥å®æ–½é˜¶çº§è·³è·ƒã€‚è¿™å¯èƒ½åŒ…æ‹¬å»ºç«‹å¼ºå¤§çš„å…³ç³»ç½‘ç»œã€æé«˜è‡ªå·±çš„æŠ€èƒ½ã€é€‰æ‹©é€‚å½“çš„æ—¶é—´å’Œåœºåˆï¼Œä»¥åŠå®æ–½é˜¶çº§è·³è·ƒã€‚\\n\\né˜¶çº§è·³è·ƒæ˜¯ä¸€ä¸ªéœ€è¦ä¸æ–­å­¦ä¹ å’Œæ”¹è¿›çš„è¿‡ç¨‹ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªéœ€è¦è€å¿ƒå’Œæ¯…åŠ›çš„è¿‡ç¨‹ã€‚åªæœ‰å¯¹äºé‚£äº›å…·æœ‰åšå®šçš„ä¿¡å¿µå’Œæ¯…åŠ›çš„äººæ¥è¯´ï¼Œé˜¶çº§è·³è·ƒæ‰èƒ½çœŸæ­£å®ç°ã€‚')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding\n",
        "å¤–éƒ¨ä¿¡æ¯ç¼–ç æˆä¸€ä¸ªé«˜ç»´å‘é‡"
      ],
      "metadata": {
        "id": "dKx53ahfSmKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #å®˜æ–¹ç¤ºä¾‹ä»£ç ï¼Œç”¨çš„OpenAIçš„adaçš„æ–‡æœ¬Embeddingæ¨¡å‹\n",
        "# #1ï¼‰ Embeding model\n",
        "# from langchain.embeddings import OpenAIEmbeddings\n",
        "# embeddings = OpenAIEmbeddings(model_name=\"ada\")\n",
        "# query_result = embeddings.embed_query(\"ä½ å¥½\")\n",
        "\n",
        "# #2) æ–‡æœ¬åˆ‡å‰²\n",
        "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# text_splitter = RecursiveCharacterTextSplitter(\n",
        "#     chunk_size=100, chunk_overlap=0\n",
        "# )\n",
        "# texts = \"\"\"é˜¶çº§è·³è·ƒæ˜¯æŒ‡ä¸€ä¸ªäººæˆ–ä¸€ä¸ªç»„ç»‡é€šè¿‡æé«˜è‡ªå·±çš„æŠ€èƒ½ã€çŸ¥è¯†å’Œé¢†å¯¼èƒ½åŠ›ï¼Œä»ä¸€ä¸ªé˜¶çº§è·¨è¶Šåˆ°å¦ä¸€ä¸ªé˜¶çº§çš„è¿‡ç¨‹ã€‚è¦å®ç°é˜¶çº§è·³è·ƒï¼Œä»¥ä¸‹æ˜¯ä¸€äº›å»ºè®®ï¼š\\n\\n1. å­¦ä¹ æ–°æŠ€èƒ½ï¼šå­¦ä¹ æ–°æŠ€èƒ½å¯ä»¥è®©äººå…·å¤‡æ–°çš„çŸ¥è¯†å’ŒæŠ€èƒ½ï¼Œä»è€Œå¢åŠ è‡ªå·±çš„ç«äº‰åŠ›ã€‚å¯ä»¥é€‰æ‹©å­¦ä¹ ä¸ç›®å‰å·¥ä½œç›¸å…³çš„æ–°æŠ€èƒ½ï¼Œæˆ–è€…å­¦ä¹ ä¸æœªæ¥å·¥ä½œç›¸å…³çš„æ–°æŠ€èƒ½ã€‚\\n\\n2. æé«˜çŸ¥è¯†æ°´å¹³ï¼šä¸æ–­å­¦ä¹ æ–°çŸ¥è¯†å¯ä»¥å¢åŠ è‡ªå·±çš„çŸ¥è¯†å‚¨å¤‡ï¼Œä»è€Œæé«˜è‡ªå·±çš„ç«äº‰åŠ›ã€‚å¯ä»¥é€šè¿‡é˜…è¯»ä¹¦ç±ã€å‚åŠ åŸ¹è®­ã€å‚ä¸çº¿ä¸Šè¯¾ç¨‹ç­‰æ–¹å¼æ¥æé«˜è‡ªå·±çš„çŸ¥è¯†æ°´å¹³ã€‚\\n\\n3. å»ºç«‹è‰¯å¥½çš„äººé™…å…³ç³»ï¼šå»ºç«‹è‰¯å¥½çš„äººé™…å…³ç³»å¯ä»¥è®©äººæ›´å®¹æ˜“å¾—åˆ°æ–°æœºä¼šï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥è·å¾—æ›´å¤šçš„æ”¯æŒå’Œå¸®åŠ©ã€‚å¯ä»¥é€šè¿‡å‚åŠ ç¤¾äº¤æ´»åŠ¨ã€å»ºç«‹äººè„‰ã€å‚åŠ ç¤¾åŒºç»„ç»‡ç­‰æ–¹å¼æ¥å»ºç«‹è‰¯å¥½çš„äººé™…å…³ç³»ã€‚\\n\\n4. æé«˜è‡ªå·±çš„é¢†å¯¼èƒ½åŠ›ï¼šé¢†å¯¼èƒ½åŠ›å¯ä»¥è®©äººæ›´å¥½åœ°ç®¡ç†è‡ªå·±çš„æ—¶é—´å’Œèµ„æºï¼Œä»è€Œæ›´å¥½åœ°å®Œæˆå·¥ä½œã€‚å¯ä»¥é€šè¿‡å‚åŠ é¢†å¯¼åŠ›è¯¾ç¨‹ã€å‚åŠ å›¢é˜Ÿå»ºè®¾æ´»åŠ¨ã€è‡ªæˆ‘åæ€ç­‰æ–¹å¼æ¥æé«˜è‡ªå·±çš„é¢†å¯¼èƒ½åŠ›ã€‚\\n\\n5. å»ºç«‹è‡ªå·±çš„å“ç‰Œï¼šå»ºç«‹è‡ªå·±çš„å“ç‰Œå¯ä»¥è®©äººæ›´å®¹æ˜“è¢«äººè®°ä½ï¼Œä»è€Œæ›´å®¹æ˜“å¾—åˆ°æ–°æœºä¼šã€‚å¯ä»¥é€šè¿‡å†™åšå®¢ã€å‘å¸ƒè§†é¢‘ã€åˆ¶ä½œç½‘ç«™ç­‰æ–¹å¼æ¥å»ºç«‹è‡ªå·±çš„å“ç‰Œã€‚\\n\\né˜¶çº§è·³è·ƒéœ€è¦é•¿æœŸçš„åŠªåŠ›å’Œä¸æ–­å­¦ä¹ ï¼Œéœ€è¦å¯¹è‡ªå·±çš„èƒ½åŠ›å’Œç›®æ ‡æœ‰æ¸…æ™°çš„è®¤è¯†ï¼Œå¹¶åˆ¶å®šæ˜ç¡®çš„è®¡åˆ’å’Œç›®æ ‡ã€‚\"\"\"\n",
        "# texts = text_splitter.create_documents([texts])\n",
        "# print(texts[0].page_content)\n",
        "\n",
        "# # 3)å…¥åº“æ£€ç´¢ï¼Œå®˜æ–¹ä½¿ç”¨çš„Pinecone,ä»–æä¾›ä¸€ä¸ªåå°ç®¡ç†ç•Œé¢ | ç”¨æˆ·éœ€æ±‚å¤ªå¤§ï¼Œä¸å¥½ç”¨äº†å·²ç»ï¼Œä¸€ç›´åŠ è½½ä¸­....\n",
        "# import pinecone\n",
        "# from langchain.vectorstores import Pinecone\n",
        "# pinecone.init(api_key=os.getenv(\"\"), enviroment=os.getenv(\"\"))\n",
        "\n",
        "# index_name = \"demo\"\n",
        "# search = Pinecone.from_documents(texts=texts, embeddings, index_name=index_name)\n",
        "# query = \"What is magical about an autoencoder?\"\n",
        "# result = search.similarity_search(query)\n",
        "\n",
        "### è¿™é‡Œä½¿ç”¨chatGLM\n",
        "# 1ï¼‰ Embedding model:  text2vec-large-chinese"
      ],
      "metadata": {
        "id": "odYd5qgFSnIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "\n",
        "class TextSpliter(CharacterTextSplitter):\n",
        "    def __init__(self, separator: str = \"\\n\\n\", **kwargs: Any):\n",
        "        super().__init__(separator, **kwargs)\n",
        "\n",
        "    def split_text(self, text: str) -> List[str]:\n",
        "        texts = text.split(\"\\n\")\n",
        "        texts = [Document(page_content=text, metadata={\"from\": \"filename or book.txt\"}) for text in texts]\n",
        "        return texts\n",
        "\n",
        "texts = response\n",
        "\n",
        "text_splitter = TextSpliter()\n",
        "texts = text_splitter.split_text(texts)\n",
        "texts1 = [text.page_content for text in texts]\n",
        "\n",
        "texts1"
      ],
      "metadata": {
        "id": "KD-lRdx-Ynxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"GanymedeNil/text2vec-large-chinese\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs={'device': \"cuda\"})\n",
        "query_result = embeddings.embed_query(\"é˜¶çº§è·³è·ƒ\")\n",
        "\n",
        "np.array(query_result).shape"
      ],
      "metadata": {
        "id": "vLkGJuZcbPUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vs_path = \"text_to_emb\"\n",
        "\n",
        "docs = embeddings.embed_documents(texts1)\n",
        "\n",
        "vector_store = FAISS.from_documents(texts, embeddings)\n",
        "vector_store.save_local(vs_path)\n",
        "\n",
        "vector_store = FAISS.load_local(vs_path, embeddings)\n",
        "related_docs_with_score = vector_store.similarity_search_with_score(query=\"é˜¶çº§è·³è·ƒ\", k=2)\n",
        "\n",
        "related_docs_with_score"
      ],
      "metadata": {
        "id": "x7vw5wD6bOLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# åŸºäºæŸ¥è¯¢åˆ°çš„çŸ¥è¯†åšprompt\n",
        "context = \"\"\n",
        "for pack in related_docs_with_score:\n",
        "    doc, socre = pack\n",
        "    content = doc.page_content\n",
        "    print(\"æ£€ç´¢åˆ°çš„çŸ¥è¯†=%s, from=%s, socre=%.3f\"%(content, doc.metadata.get(\"from\"), socre))\n",
        "    context += content"
      ],
      "metadata": {
        "id": "J8-n7QC2dS8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# é‡æ–°é…ç½®ä¸€ä¸ªåŸºäºä¸Šä¸‹æ–‡çš„æ¨¡æ¿åœ¨æ¥è°ƒä¸‹è¯­è¨€æ¨¡å‹\n",
        "template = \"å·²çŸ¥{context}, è¯·ç»™æˆ‘è§£é‡Šä¸€ä¸‹{query}çš„æ„æ€?\"\n",
        "promptTem = PromptTemplate(input_variables=[\"context\", \"query\"], template=template)\n",
        "chain = DemoChain(llm=llm, prompt=promptTem, history=[])\n",
        "print(\"-\"*80)\n",
        "print(chain.run(query=\"é˜¶çº§è·³è·ƒ\", context=context, history=[]))\n",
        "print(\"-\"*80)"
      ],
      "metadata": {
        "id": "I-wjt8rWf2tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings HuggingFace"
      ],
      "metadata": {
        "id": "5AZA3Af7U4CR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "pcpxG5aA_mZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding æŒä¹…åŒ–"
      ],
      "metadata": {
        "id": "_gMz_XnFVBa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "HBg_cyR1_oC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### å¯¹è¶…é•¿æ–‡æœ¬è¿›è¡Œæ€»ç»“"
      ],
      "metadata": {
        "id": "uQ_CFNbDyAZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "import IPython\n",
        "import sentence_transformers\n",
        "\n",
        "\n",
        "embedding_model_dict = {\n",
        "    \"ernie-tiny\": \"nghuyong/ernie-3.0-nano-zh\",\n",
        "    \"ernie-base\": \"nghuyong/ernie-3.0-base-zh\",\n",
        "    \"text2vec\": \"GanymedeNil/text2vec-large-chinese\",\n",
        "    \"text2vec2\":\"uer/sbert-base-chinese-nli\",\n",
        "    \"text2vec3\":\"shibing624/text2vec-base-chinese\",\n",
        "}\n",
        "\n",
        "EMBEDDING_MODEL = \"text2vec3\"\n",
        "# åˆå§‹åŒ– hugginFace çš„ embeddings å¯¹è±¡\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_dict[EMBEDDING_MODEL], )\n",
        "embeddings.client = sentence_transformers.SentenceTransformer(\n",
        "        embeddings.model_name, device='mps')\n"
      ],
      "metadata": {
        "id": "o_TcsCeAU6om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "# åˆå§‹åŒ–åŠ è½½å™¨\n",
        "db = Chroma.from_documents(split_docs, embeddings,persist_directory=\"./news_test\")\n",
        "# æŒä¹…åŒ–\n",
        "db.persist()\n",
        "\n",
        "db = Chroma(persist_directory=\"./news_test\", embedding_function=embeddings)\n"
      ],
      "metadata": {
        "id": "FO6DX4PHVFGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "db = FAISS.from_documents(split_docs, embeddings)\n",
        "db.save_local(\"./news_test\")\n",
        "\n",
        "db = FAISS.load_local(\"./news_test\",embeddings=embeddings)"
      ],
      "metadata": {
        "id": "FL1nNyQOVQTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "import IPython\n",
        "\n",
        "retriever = db.as_retriever()\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
        "# è¿›è¡Œé—®ç­”\n",
        "query = \"2022å¹´è…¾è®¯è¥æ”¶å¤šå°‘\"\n",
        "print(qa.run(query))"
      ],
      "metadata": {
        "id": "pwjDxF44V64p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, List, Mapping, Optional\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain.llms.base import LLM\n",
        "import re\n",
        "\n",
        "class TfboyLLM(LLM):\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "    ) -> str:\n",
        "        print(\"é—®é¢˜:\",prompt)\n",
        "        pattern = re.compile(r'^.*(\\d+[*/+-]\\d+).*$')\n",
        "        match = pattern.search(prompt)\n",
        "        if match:\n",
        "            result = eval(match.group(1))\n",
        "        elif \"ï¼Ÿ\" in prompt:\n",
        "            rep_args = {\"æˆ‘\":\"ä½ \", \"ä½ \":\"æˆ‘\", \"å—\":\"\", \"ï¼Ÿ\":\"ï¼\"}\n",
        "            result = [(rep_args[c] if c in rep_args else c) for c in list(prompt)]\n",
        "            result = ''.join(result)\n",
        "        else:\n",
        "            result = \"å¾ˆæŠ±æ­‰ï¼Œè¯·æ¢ä¸€ç§é—®æ³•ã€‚æ¯”å¦‚ï¼š1+1ç­‰äºå‡ \"\n",
        "        return result\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        return {}\n",
        "\n"
      ],
      "metadata": {
        "id": "4GQgTpN_7xyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = TfboyLLM()\n",
        "print(\"ç­”æ¡ˆ:\",llm(\"æˆ‘èƒ½é—®ä½ é—®é¢˜å—ï¼Ÿ\"))"
      ],
      "metadata": {
        "id": "6-ra4sWw8AnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# å®ä¾‹"
      ],
      "metadata": {
        "id": "KfgmWXlx_Pws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ä¿¡æ¯æŠ½å–"
      ],
      "metadata": {
        "id": "aUWP4xNR_Uuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, List, Mapping, Optional\n",
        "\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain.llms.base import LLM\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "\n",
        "\n",
        "class CustomLLM(LLM):\n",
        "    def __init__(self, model_name, quantization_bit=4):\n",
        "        print('----1', model_name)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "        print('----2', model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name, trust_remote_code=True).half().cuda().eval()\n",
        "        # self.model = self.model.quantize(quantization_bit)\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "    ) -> str:\n",
        "        response, history = self.model.chat(self.tokenizer, prompt)\n",
        "        return response, history\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        return {}\n"
      ],
      "metadata": {
        "id": "lZxCyTLZF3ur"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = CustomLLM(model_name=\"THUDM/chatglm-6B-int4\")\n",
        "llm(\"This is a foobar thing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "mXAD14HCGJY9",
        "outputId": "4a15b5bc-98e9-4526-d0fe-4142f64c22b4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----1 THUDM/chatglm-6B-int4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-eb0caf3d6d81>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomLLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"THUDM/chatglm-6B-int4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This is a foobar thing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-699754725a19>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name, quantization_bit)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantization_bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'----1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'----2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.cpython-310-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__setattr__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: \"CustomLLM\" object has no field \"tokenizer\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "\n",
        "\n",
        "text = \"é˜¿å°”èŒ¨æµ·é»˜ç—…(Alzheimer's disease, AD),ä¿—ç§°è€å¹´ç—´å‘†ç—‡,æ˜¯ä¸€ç§å…¨èº«æ€§ç¥ç»é€€è¡Œæ€§ç–¾ç—…ï¼Œå®ƒæ˜¯ç”±å¤§è„‘ç¥ç»é€€è¡Œæ€§å˜æ€§å¼•èµ·çš„ï¼Œ\\\n",
        "ä¸»è¦è¡¨ç°ä¸ºè®°å¿†åŠ›å‡é€€ã€æ€ç»´èƒ½åŠ›å‡é€€ã€è¡Œä¸ºå˜åŒ–ç­‰ã€‚é˜¿å°”èŒ¨æµ·é»˜ç—…çš„åŸå› å°šä¸ååˆ†æ¸…æ¥šï¼Œä½†æ˜¯ç ”ç©¶è¡¨æ˜ï¼Œé˜¿å°”èŒ¨æµ·é»˜ç—…å¯èƒ½ä¸é—ä¼ å› ç´ ã€ç¯å¢ƒå› ç´ ã€\\\n",
        "è¥å…»ä¸è‰¯ã€å‹åŠ›è¿‡å¤§ã€ä¸è‰¯ç”Ÿæ´»ä¹ æƒ¯ç­‰æœ‰å…³ã€‚æ ¹æ®ä¸–ç•Œå«ç”Ÿç»„ç»‡çš„ç»Ÿè®¡æ•°æ®ï¼Œå…¨çƒæœ‰è¶…è¿‡4700ä¸‡äººæ‚£æœ‰é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼Œå…¶ä¸­ç¾å›½æœ‰è¶…è¿‡600ä¸‡äººæ‚£æœ‰é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼Œ\\\n",
        "æ¬§æ´²æœ‰è¶…è¿‡1000ä¸‡äººæ‚£æœ‰é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼Œäºšæ´²æœ‰è¶…è¿‡2500ä¸‡äººæ‚£æœ‰é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼Œå…¶ä¸­ä¸­å›½æœ‰è¶…è¿‡1000ä¸‡äººæ‚£æœ‰é˜¿å°”èŒ¨æµ·é»˜ç—…ã€‚é˜¿å°”èŒ¨æµ·é»˜ç—…çš„å‘ç—…ç‡ä¸å¹´é¾„æœ‰å…³ï¼Œ\\\n",
        "éšç€å¹´é¾„çš„å¢é•¿è€Œå¢åŠ ï¼Œ65å²ä»¥ä¸Šçš„äººç¾¤ä¸ºä¸»è¦å—å®³ç¾¤ä½“ï¼Œå æ¯”é«˜è¾¾80%ï¼Œå…¶ä¸­45-65å²çš„äººç¾¤å æ¯”ä¸º15%ï¼Œ20-45å²çš„äººç¾¤å æ¯”ä¸º5%ã€‚65å²ä»¥ä¸Šçš„äººç¾¤å‘ç—…ç‡çº¦ä¸º10%ï¼Œ\\\n",
        "75å²ä»¥ä¸Šçš„äººç¾¤å‘ç—…ç‡çº¦ä¸º20%ï¼Œ85å²ä»¥ä¸Šçš„äººç¾¤å‘ç—…ç‡çº¦ä¸º30%ã€‚æ ¹æ®ç»Ÿè®¡ï¼Œç”·æ€§æ‚£ç—…ç‡é«˜äºå¥³æ€§ï¼Œç”·æ€§æ‚£ç—…æ¯”ä¾‹ä¸º1.4ï¼š1ï¼Œå³ç”·æ€§æ‚£ç—…ç‡æ¯”å¥³æ€§é«˜å‡º40%ã€‚\\\n",
        "æ ¹æ®ç»Ÿè®¡ï¼Œé˜¿å°”èŒ¨æµ·é»˜ç—…åœ¨ä¸åŒçš„äººç§ä¸­åˆ†å¸ƒæƒ…å†µä¹Ÿæœ‰æ‰€ä¸åŒã€‚ç™½äººæ‚£ç—…ç‡æœ€é«˜ï¼Œå æ€»æ‚£ç—…ç‡çš„70%ï¼Œé»‘äººæ‚£ç—…ç‡æ¬¡ä¹‹ï¼Œå æ€»æ‚£ç—…ç‡çš„20%ï¼Œ\\\n",
        "å…¶ä»–å°‘æ•°æ°‘æ—æ‚£ç—…ç‡æœ€ä½ï¼Œå æ€»æ‚£ç—…ç‡çš„10%ã€‚é˜¿å°”èŒ¨æµ·é»˜ç—…åœ¨ä¸åŒçš„é¥®é£Ÿä¹ æƒ¯ä¸­åˆ†å¸ƒæƒ…å†µä¹Ÿæœ‰æ‰€ä¸åŒã€‚ç»´ç”Ÿç´ B12ç¼ºä¹çš„äººç¾¤æ‚£ç—…ç‡æ›´é«˜ï¼Œ\\\n",
        "è€Œå‡è¡¡è†³é£Ÿçš„äººç¾¤æ‚£ç—…ç‡è¾ƒä½ã€‚é˜¿å°”èŒ¨æµ·é»˜ç—…ä¸ä»…ä¼šç»™æ‚£è€…å¸¦æ¥è®°å¿†åŠ›å‡é€€ã€æ€ç»´èƒ½åŠ›å‡é€€ã€è¡Œä¸ºå˜åŒ–ç­‰ç—‡çŠ¶ï¼Œè¿˜ä¼šç»™æ‚£è€…çš„å®¶åº­å¸¦æ¥å·¨å¤§çš„å¿ƒç†è´Ÿæ‹…ã€‚\\\n",
        "å› æ­¤ï¼Œæ‚£è€…åº”å°½å¿«å°±åŒ»ï¼ŒåŠæ—¶è¿›è¡Œæ²»ç–—ã€‚æ²»ç–—é˜¿å°”èŒ¨æµ·é»˜ç—…çš„æ–¹æ³•æœ‰è¯ç‰©æ²»ç–—ã€è¡Œä¸ºæ²»ç–—ã€è®¤çŸ¥è¡Œä¸ºæ²»ç–—ç­‰ï¼Œå…·ä½“æ²»ç–—æ–¹æ¡ˆè¦æ ¹æ®æ‚£è€…çš„å…·ä½“æƒ…å†µè€Œå®šã€‚\"\n",
        "\n",
        "#åˆ›å»ºæ¨¡æ¿\n",
        "fact_extraction_prompt = PromptTemplate(\n",
        "    input_variables=[\"text_input\"],\n",
        "    template=\"ä»ä¸‹é¢çš„æœ¬æ–‡ä¸­æå–å…³é”®äº‹å®ã€‚å°½é‡ä½¿ç”¨æ–‡æœ¬ä¸­çš„ç»Ÿè®¡æ•°æ®æ¥è¯´æ˜äº‹å®:\\n\\n {text_input}\"\n",
        ")\n",
        "\n",
        "#å®šä¹‰chain\n",
        "fact_extraction_chain = LLMChain(llm=llm, prompt=fact_extraction_prompt)\n",
        "facts = fact_extraction_chain.run(text)\n",
        "print(facts)"
      ],
      "metadata": {
        "id": "Otck1zjw_WXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.chains.base import Chain\n",
        "\n",
        "from typing import Dict, List\n",
        "\n",
        "\n",
        "class ConcatenateChain(Chain):\n",
        "    chain_1: LLMChain\n",
        "    chain_2: LLMChain\n",
        "\n",
        "    @property\n",
        "    def input_keys(self) -> List[str]:\n",
        "        # Union of the input keys of the two chains.\n",
        "        all_input_vars = set(self.chain_1.input_keys).union(set(self.chain_2.input_keys))\n",
        "        return list(all_input_vars)\n",
        "\n",
        "    @property\n",
        "    def output_keys(self) -> List[str]:\n",
        "        return ['concat_output']\n",
        "\n",
        "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
        "        output_1 = self.chain_1.run(inputs)\n",
        "        output_2 = self.chain_2.run(inputs)\n",
        "        return {'concat_output': output_1 + output_2}\n",
        "\n",
        "prompt_1 = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a good name for a company that makes {product}?\",\n",
        ")\n",
        "chain_1 = LLMChain(llm=llm, prompt=prompt_1)\n",
        "\n",
        "prompt_2 = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a good slogan for a company that makes {product}?\",\n",
        ")\n",
        "chain_2 = LLMChain(llm=llm, prompt=prompt_2)\n",
        "\n",
        "concat_chain = ConcatenateChain(chain_1=chain_1, chain_2=chain_2)\n",
        "concat_output = concat_chain.run(\"colorful socks\")\n",
        "print(f\"Concatenated output:\\n{concat_output}\")\n",
        "\n",
        "\n",
        "concat_chain = ConcatenateChain(chain_1=chain_1, chain_2=chain_2)\n",
        "concat_output = concat_chain.run(\"colorful socks\")\n"
      ],
      "metadata": {
        "id": "LwFPex-UEX31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"é˜¿å°”èŒ¨æµ·é»˜ç—…(Alzheimer's disease, AD),ä¿—ç§°è€å¹´ç—´å‘†ç—‡,æ˜¯ä¸€ç§å…¨èº«æ€§ç¥ç»é€€è¡Œæ€§ç–¾ç—…ï¼Œå®ƒæ˜¯ç”±å¤§è„‘ç¥ç»é€€è¡Œæ€§å˜æ€§å¼•èµ·çš„ï¼Œ\\\n",
        "ä¸»è¦è¡¨ç°ä¸ºè®°å¿†åŠ›å‡é€€ã€æ€ç»´èƒ½åŠ›å‡é€€ã€è¡Œä¸ºå˜åŒ–ç­‰ã€‚é˜¿å°”èŒ¨æµ·é»˜ç—…çš„åŸå› å°šä¸ååˆ†æ¸…æ¥šï¼Œä½†æ˜¯ç ”ç©¶è¡¨æ˜ï¼Œé˜¿å°”èŒ¨æµ·é»˜ç—…å¯èƒ½ä¸é—ä¼ å› ç´ ã€ç¯å¢ƒå› ç´ ã€\\\n",
        "è¥å…»ä¸è‰¯ã€å‹åŠ›è¿‡å¤§ã€ä¸è‰¯ç”Ÿæ´»ä¹ æƒ¯ç­‰æœ‰å…³ã€‚æ ¹æ®ä¸–ç•Œå«ç”Ÿç»„ç»‡çš„ç»Ÿè®¡æ•°æ®ï¼Œå…¨çƒæœ‰è¶…è¿‡4700ä¸‡äººæ‚£æœ‰é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼Œå…¶ä¸­ç¾å›½æœ‰è¶…è¿‡600ä¸‡äººæ‚£æœ‰é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼Œ\\\n",
        "æ¬§æ´²æœ‰è¶…è¿‡1000ä¸‡äººæ‚£æœ‰é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼Œäºšæ´²æœ‰è¶…è¿‡2500ä¸‡äººæ‚£æœ‰é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼Œå…¶ä¸­ä¸­å›½æœ‰è¶…è¿‡1000ä¸‡äººæ‚£æœ‰é˜¿å°”èŒ¨æµ·é»˜ç—…ã€‚é˜¿å°”èŒ¨æµ·é»˜ç—…çš„å‘ç—…ç‡ä¸å¹´é¾„æœ‰å…³ï¼Œ\\\n",
        "éšç€å¹´é¾„çš„å¢é•¿è€Œå¢åŠ ï¼Œ65å²ä»¥ä¸Šçš„äººç¾¤ä¸ºä¸»è¦å—å®³ç¾¤ä½“ï¼Œå æ¯”é«˜è¾¾80%ï¼Œå…¶ä¸­45-65å²çš„äººç¾¤å æ¯”ä¸º15%ï¼Œ20-45å²çš„äººç¾¤å æ¯”ä¸º5%ã€‚65å²ä»¥ä¸Šçš„äººç¾¤å‘ç—…ç‡çº¦ä¸º10%ï¼Œ\\\n",
        "75å²ä»¥ä¸Šçš„äººç¾¤å‘ç—…ç‡çº¦ä¸º20%ï¼Œ85å²ä»¥ä¸Šçš„äººç¾¤å‘ç—…ç‡çº¦ä¸º30%ã€‚æ ¹æ®ç»Ÿè®¡ï¼Œç”·æ€§æ‚£ç—…ç‡é«˜äºå¥³æ€§ï¼Œç”·æ€§æ‚£ç—…æ¯”ä¾‹ä¸º1.4ï¼š1ï¼Œå³ç”·æ€§æ‚£ç—…ç‡æ¯”å¥³æ€§é«˜å‡º40%ã€‚\\\n",
        "æ ¹æ®ç»Ÿè®¡ï¼Œé˜¿å°”èŒ¨æµ·é»˜ç—…åœ¨ä¸åŒçš„äººç§ä¸­åˆ†å¸ƒæƒ…å†µä¹Ÿæœ‰æ‰€ä¸åŒã€‚ç™½äººæ‚£ç—…ç‡æœ€é«˜ï¼Œå æ€»æ‚£ç—…ç‡çš„70%ï¼Œé»‘äººæ‚£ç—…ç‡æ¬¡ä¹‹ï¼Œå æ€»æ‚£ç—…ç‡çš„20%ï¼Œ\\\n",
        "å…¶ä»–å°‘æ•°æ°‘æ—æ‚£ç—…ç‡æœ€ä½ï¼Œå æ€»æ‚£ç—…ç‡çš„10%ã€‚é˜¿å°”èŒ¨æµ·é»˜ç—…åœ¨ä¸åŒçš„é¥®é£Ÿä¹ æƒ¯ä¸­åˆ†å¸ƒæƒ…å†µä¹Ÿæœ‰æ‰€ä¸åŒã€‚ç»´ç”Ÿç´ B12ç¼ºä¹çš„äººç¾¤æ‚£ç—…ç‡æ›´é«˜ï¼Œ\\\n",
        "è€Œå‡è¡¡è†³é£Ÿçš„äººç¾¤æ‚£ç—…ç‡è¾ƒä½ã€‚é˜¿å°”èŒ¨æµ·é»˜ç—…ä¸ä»…ä¼šç»™æ‚£è€…å¸¦æ¥è®°å¿†åŠ›å‡é€€ã€æ€ç»´èƒ½åŠ›å‡é€€ã€è¡Œä¸ºå˜åŒ–ç­‰ç—‡çŠ¶ï¼Œè¿˜ä¼šç»™æ‚£è€…çš„å®¶åº­å¸¦æ¥å·¨å¤§çš„å¿ƒç†è´Ÿæ‹…ã€‚\\\n",
        "å› æ­¤ï¼Œæ‚£è€…åº”å°½å¿«å°±åŒ»ï¼ŒåŠæ—¶è¿›è¡Œæ²»ç–—ã€‚æ²»ç–—é˜¿å°”èŒ¨æµ·é»˜ç—…çš„æ–¹æ³•æœ‰è¯ç‰©æ²»ç–—ã€è¡Œä¸ºæ²»ç–—ã€è®¤çŸ¥è¡Œä¸ºæ²»ç–—ç­‰ï¼Œå…·ä½“æ²»ç–—æ–¹æ¡ˆè¦æ ¹æ®æ‚£è€…çš„å…·ä½“æƒ…å†µè€Œå®šã€‚\"\n",
        "\n",
        "\n",
        "#åˆ›å»ºæ¨¡æ¿\n",
        "fact_extraction_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=\"ä»ä¸‹é¢çš„æœ¬æ–‡ä¸­æå–å…³é”®äº‹å®ã€‚å°½é‡ä½¿ç”¨æ–‡æœ¬ä¸­çš„ç»Ÿè®¡æ•°æ®æ¥è¯´æ˜äº‹å®:\\n\\n {query}\"\n",
        ")\n",
        "\n",
        "fact_extraction_chain = DemoChain(llm=llm, prompt=fact_extraction_prompt, history=[])\n",
        "response, history = fact_extraction_chain.run(query=text, history=[])\n",
        "print(response, history)"
      ],
      "metadata": {
        "id": "_6Ah9BnWAabw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doctor_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=\"ä½ æ˜¯ç¥ç»å†…ç§‘åŒ»ç”Ÿã€‚æ ¹æ®ä»¥ä¸‹é˜¿å°”èŒ¨æµ·é»˜ç—…çš„äº‹å®ç»Ÿè®¡åˆ—è¡¨ï¼Œä¸ºæ‚¨çš„ç—…äººå†™ä¸€ä¸ªç®€çŸ­çš„é¢„é˜²é˜¿å°”èŒ¨æµ·é»˜ç—…çš„å»ºè®®ã€‚ ä¸è¦é—æ¼å…³é”®ä¿¡æ¯ï¼š\\n\\n {query}\"\n",
        ")\n",
        "\n",
        "doctor_chain = DemoChain(llm=llm, prompt=doctor_prompt, history=history)\n",
        "response, history = doctor_chain.run(query=response, history=history)\n",
        "print(response, history)"
      ],
      "metadata": {
        "id": "AG0-mJoOBbV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain, SequentialChain\n",
        "\n",
        "\n",
        "#å®šä¹‰SimpleSequentialChain\n",
        "full_chain = SimpleSequentialChain(chains=[fact_extraction_chain, doctor_chain], verbose=True)\n",
        "response, history = full_chain.run(text)\n",
        "print(response, history)\n"
      ],
      "metadata": {
        "id": "em9SQwT2B19B"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5AZA3Af7U4CR",
        "_gMz_XnFVBa6",
        "uQ_CFNbDyAZM"
      ],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO/hsJxZLuTFLQlV1RjRiBf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}