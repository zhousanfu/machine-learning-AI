{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhousanfu/machine-learning-demo/blob/master/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9E_%E5%9F%BA%E4%BA%8E%E8%AE%BA%E6%96%87%E6%91%98%E8%A6%81%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8A%BD%E5%8F%96%E6%8C%91%E6%88%98%E8%B5%9B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTh6x7SiZ3Cw"
      },
      "source": [
        "# ç§‘å¤§è®¯é£ åŸºäºè®ºæ–‡æ‘˜è¦çš„æ–‡æœ¬åˆ†ç±»ä¸å…³é”®è¯æŠ½å–æŒ‘æˆ˜èµ›[ğŸ”—](https://challenge.xfyun.cn/topic/info?type=abstract-of-the-paper&option=ssgy&ch=F5ZbQiB)\n",
        "\n",
        "ä¸€ã€èµ›äº‹èƒŒæ™¯\n",
        "\n",
        "åŒ»å­¦é¢†åŸŸçš„æ–‡çŒ®åº“ä¸­è•´å«äº†ä¸°å¯Œçš„ç–¾ç—…è¯Šæ–­å’Œæ²»ç–—ä¿¡æ¯ï¼Œå¦‚ä½•é«˜æ•ˆåœ°ä»æµ·é‡æ–‡çŒ®ä¸­æå–å…³é”®ä¿¡æ¯ï¼Œè¿›è¡Œç–¾ç—…è¯Šæ–­å’Œæ²»ç–—æ¨èï¼Œå¯¹äºä¸´åºŠåŒ»ç”Ÿå’Œç ”ç©¶äººå‘˜å…·æœ‰é‡è¦æ„ä¹‰ã€‚\n",
        "\n",
        "äºŒã€èµ›äº‹ä»»åŠ¡\n",
        "\n",
        "æœ¬ä»»åŠ¡åˆ†ä¸ºä¸¤ä¸ªå­ä»»åŠ¡ï¼š\n",
        "æœºå™¨é€šè¿‡å¯¹è®ºæ–‡æ‘˜è¦ç­‰ä¿¡æ¯çš„ç†è§£ï¼Œåˆ¤æ–­è¯¥è®ºæ–‡æ˜¯å¦å±äºåŒ»å­¦é¢†åŸŸçš„æ–‡çŒ®ã€‚\n",
        "æå–å‡ºè¯¥è®ºæ–‡æ ‡é¢˜ã€ä½œè€…ã€æ‘˜è¦ã€å…³é”®è¯ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqrpB-VDIB3N"
      },
      "source": [
        "**åŠ è½½ç½‘ç›˜æ–‡ä»¶**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayHlB1zPaIEm",
        "outputId": "093b6590-2bea-432a-c0da-5de79dcc7e46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!cp /content/drive/MyDrive/Data/ç§‘å¤§è®¯é£æ¯”èµ›/* ./data"
      ],
      "metadata": {
        "id": "yZ806f_IPrO6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2f7EK5vWgZU"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[tensorflow]\n",
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bN4PStWf2WHD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, TFAutoModel, TFBertForSequenceClassification, TFAutoModelForTokenClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4h19VLOZ3C1"
      },
      "source": [
        "## ä»»åŠ¡ä¸€: åˆ†ç±»"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXWKS4G0hmWD"
      },
      "source": [
        "### è¶…å‚æ•°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jS0uP5d_wVEf"
      },
      "outputs": [],
      "source": [
        "vocab_size = 10000  # è¯æ±‡è¡¨å¤§å°\n",
        "embedding_dim = 100  # è¯å‘é‡ç»´åº¦\n",
        "hidden_size = 128  # LSTMéšè—å±‚å¤§å°\n",
        "max_sequence_length = 128\n",
        "batch_size = 16\n",
        "\n",
        "num_classes = 2  # åˆ†ç±»ä»»åŠ¡ç±»åˆ«æ•°ï¼Œè¿™é‡Œä¸ºåŒ»å­¦æ–‡çŒ®å’ŒéåŒ»å­¦æ–‡çŒ®\n",
        "num_extraction = 10  # æå–ä»»åŠ¡æå–çš„å…³é”®è¯æ•°ç›®\n",
        "epochs_classes = 10\n",
        "epochs_keywords = 10\n",
        "classification_learning_rate = 0.001\n",
        "extraction_learning_rate = 0.001\n",
        "weight_decay = 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### æ•°æ®é¢„å¤„ç†"
      ],
      "metadata": {
        "id": "G-f-So7O3RUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# åŠ è½½æ•°æ®å’Œæ ‡ç­¾\n",
        "data = pd.read_csv('data/train.csv')[:100].to_dict(orient='list')\n",
        "sentences = data['abstract']\n",
        "labels = data['label']\n",
        "\n",
        "# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
        "train_data, eval_data, train_labels, eval_labels = train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# åˆå§‹åŒ–BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# ç¼–ç è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„è¾“å…¥æ–‡æœ¬\n",
        "train_encodeds = tokenizer.batch_encode_plus(train_data, truncation=True, padding='max_length', max_length=max_sequence_length, return_tensors='tf')\n",
        "eval_encodeds = tokenizer.batch_encode_plus(eval_data, truncation=True, padding='max_length', max_length=max_sequence_length, return_tensors='tf')\n",
        "# input_ids = tf.convert_to_tensor(encodeds['input_ids'])\n",
        "# attention_mask = tf.convert_to_tensor(encodeds['attention_mask'])\n",
        "# labels = tf.convert_to_tensor(labels)\n",
        "\n",
        "# è½¬æ¢ä¸ºTensorFlow Datasetæ ¼å¼\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices( \\\n",
        "    ({'input_ids': train_encodeds['input_ids'], 'attention_mask': train_encodeds['attention_mask']}, train_labels) \\\n",
        "    ).shuffle(num_classes).batch(batch_size)\n",
        "eval_dataset = tf.data.Dataset.from_tensor_slices( \\\n",
        "    ({'input_ids': eval_encodeds['input_ids'], 'attention_mask': eval_encodeds['attention_mask']}, eval_labels) \\\n",
        "    ).shuffle(num_classes).batch(batch_size)\n",
        "\n",
        "# æµ‹è¯•æ•°æ®é›†\n",
        "data = pd.read_csv('data/test.csv')[:100].to_dict(orient='list')\n",
        "sentences = data['abstract']\n",
        "\n",
        "test_encodeds = tokenizer.batch_encode_plus(sentences, truncation=True, padding='max_length', max_length=max_sequence_length, return_tensors='tf')\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices( \\\n",
        "    ({'input_ids': train_encodeds['input_ids'], 'attention_mask': train_encodeds['attention_mask']}) \\\n",
        "    ).shuffle(num_classes).batch(batch_size)\n",
        "\n",
        "test_dataset"
      ],
      "metadata": {
        "id": "YC-CsEC2Jkzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF11TCKUjUdS"
      },
      "source": [
        "### æ„å»ºæ¨¡å‹"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# å®šä¹‰TFBertForSequenceClassificationæ¨¡å‹\n",
        "classes_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)\n",
        "\n",
        "# å®šä¹‰ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=classification_learning_rate)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# å®šä¹‰è¯„ä¼°æŒ‡æ ‡\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')\n",
        "eval_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('eval_accuracy')\n",
        "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
        "eval_loss = tf.keras.metrics.Mean('eval_loss', dtype=tf.float32)\n",
        "\n",
        "@tf.function\n",
        "def train_step(inputs, labels):\n",
        "    predictions = None\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        outputs = classes_model(inputs, training=True)[0]\n",
        "        loss_value = loss(labels, outputs)\n",
        "        predictions = tf.argmax(outputs, axis=1)\n",
        "\n",
        "    gradients = tape.gradient(loss_value, classes_model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, classes_model.trainable_variables))\n",
        "\n",
        "    train_accuracy(labels, outputs)\n",
        "    train_loss(loss_value)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "@tf.function\n",
        "def eval_step(inputs, labels):\n",
        "    outputs = classes_model(inputs, training=False)[0]\n",
        "    loss_value = loss(labels, outputs)\n",
        "\n",
        "    eval_accuracy(labels, outputs)\n",
        "    eval_loss(loss_value)\n",
        "\n",
        "    predictions = tf.argmax(outputs, axis=1)\n",
        "\n",
        "    return predictions\n",
        "\n"
      ],
      "metadata": {
        "id": "Nvl6-sK_I9pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### è®­ç»ƒæ¨¡å‹"
      ],
      "metadata": {
        "id": "o8HhloKHvgMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# è®­ç»ƒæ¨¡å‹\n",
        "for epoch in range(3):\n",
        "    train_accuracy.reset_states()\n",
        "    train_loss.reset_states()\n",
        "    eval_accuracy.reset_states()\n",
        "    eval_loss.reset_states()\n",
        "\n",
        "    train_predictions = []\n",
        "    eval_predictions = []\n",
        "    train_f1 = 0\n",
        "    eval_f1 = 0\n",
        "\n",
        "    for batch_inputs, batch_labels in train_dataset:\n",
        "        predictions = train_step(batch_inputs, batch_labels)\n",
        "        train_predictions.extend(predictions)\n",
        "\n",
        "    for batch_inputs, batch_labels in eval_dataset:\n",
        "        predictions = eval_step(batch_inputs, batch_labels)\n",
        "        eval_predictions.extend(predictions)\n",
        "\n",
        "    train_f1 = f1_score(train_labels, train_predictions)\n",
        "    eval_f1 = f1_score(eval_labels, eval_predictions)\n",
        "\n",
        "    print('Epoch {}: \\nè®­ç»ƒ: Loss: {:.4f}, Accuracy: {:.4f}, F1: {:.4f}, \\néªŒè¯: Loss: {:.4f}, Accuracy: {:.4f}, F1: {:.4f},'.format(\n",
        "        epoch + 1, train_loss.result(), train_accuracy.result(), train_f1, eval_loss.result(), eval_accuracy.result(), eval_f1\n",
        "    ))"
      ],
      "metadata": {
        "id": "eED1ZgT7KZQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### æ¨¡å‹é¢„æµ‹"
      ],
      "metadata": {
        "id": "qzdxn5gG4bww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = classes_model.predict(test_dataset)\n",
        "probabilities = tf.nn.softmax(outputs.logits, axis=1)\n",
        "\n",
        "for i in probabilities:\n",
        "    predicted_label = tf.argmax([i], axis=1).numpy()[0]\n",
        "    print(\"é¢„æµ‹æ ‡ç­¾:\", predicted_label, i.numpy())\n",
        "    break"
      ],
      "metadata": {
        "id": "E57UJvbhhH5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ä»»åŠ¡äºŒ: å‘½åå®ä½“æŠ½å–"
      ],
      "metadata": {
        "id": "I73UWTdXWrUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_addons transformers\n",
        "!pip install keras-bert\n",
        "!pip install bert-for-tf2\n",
        "!pip install tf2crf"
      ],
      "metadata": {
        "id": "9SS26xDYfdiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from transformers import TFBertModel, BertTokenizer, TFAutoModel, TFBertForSequenceClassification, TFAutoModelForTokenClassification\n",
        "\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import keras\n",
        "# from tf.keras.models import Model, Input, Sequential\n",
        "# from tf.keras.layers import GRU, LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda, BatchNormalization, Add\n",
        "# from tf.keras.utils import to_categorical\n",
        "# from tf.keras.callbacks import CSVLogger\n",
        "# from tf.keras.optimizers import Adam\n",
        "# from tf.keras_bert import load_trained_model_from_checkpoint\n",
        "from tf2crf import CRF, ModelWithCRFLoss\n",
        "import bert\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# åŠ è½½é¢„è®­ç»ƒçš„BERTæ¨¡å‹\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# è·å–BERTçš„tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "ZMTwhGViimD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 512\n",
        "hidden_size = 100\n",
        "num_tags = 4\n",
        "lstm_units = 64  # BiLSTMå•å…ƒæ•°é‡\n",
        "num_words = 10000  # è¯æ±‡è¡¨å¤§å°\n",
        "embedding_dim = 100  # BERTåµŒå…¥ç»´åº¦"
      ],
      "metadata": {
        "id": "ye3YJWvWMo3l"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### æ•°æ®é¢„å¤„ç†\n",
        "\n",
        "é¦–å…ˆï¼Œéœ€è¦å‡†å¤‡ä¸€ä¸ªå«æœ‰å®ä½“æ ‡æ³¨çš„æ–‡æœ¬è¯­æ–™åº“\n",
        "å¯¹äºæ¯ä¸ªæ–‡æœ¬ï¼Œéœ€è¦å°†å…¶åˆ†æˆå•ç‹¬çš„å¥å­ï¼Œå¹¶å°†æ¯ä¸ªå¥å­åˆ†è¯ã€‚\n",
        "å¯¹äºæ¯ä¸ªè¯ï¼Œéœ€è¦å°†å…¶è½¬åŒ–ä¸ºå¯¹åº”çš„BERTè¯è¡¨ä¸­çš„IDã€‚\n",
        "å¯¹äºæ ‡æ³¨åºåˆ—ï¼Œéœ€è¦å°†å…¶è½¬åŒ–ä¸ºBIOæ ¼å¼ï¼ˆå³Beginï¼ŒInsideï¼ŒOutsideï¼‰ã€‚"
      ],
      "metadata": {
        "id": "8YqeKblIv8QS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# æ•°æ®é¢„å¤„ç†\n",
        "def preprocess_data(texts, labels, tokenizer, label2id, max_length):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    encoded_labels = []\n",
        "\n",
        "    for text, label in zip(texts, labels):\n",
        "        # åˆ†è¯ï¼Œå¹¶æ·»åŠ ç‰¹æ®Šæ ‡è®°[CLS]å’Œ[SEP]ï¼Œå¹¶è¿›è¡Œidç¼–ç å’Œæ³¨æ„åŠ›æ©ç ç”Ÿæˆ\n",
        "        input_encode = tokenizer.encode_plus(text, add_special_tokens=True, max_length=max_length,\n",
        "                                             padding='max_length', truncation=True, return_token_type_ids=False)\n",
        "        input_ids.append(input_encode['input_ids'])\n",
        "        attention_masks.append(input_encode['attention_mask'])\n",
        "\n",
        "        # ç¼–ç æ ‡ç­¾åºåˆ—\n",
        "        encoded_label = [label2id[l] for l in label]\n",
        "        while len(encoded_label) < max_length:\n",
        "            encoded_label.append(1)\n",
        "        encoded_labels.append(encoded_label)\n",
        "\n",
        "    return input_ids, attention_masks, encoded_labels\n",
        "\n",
        "# æ„å»ºæ ‡ç­¾å­—å…¸\n",
        "def build_label_dict(labels):\n",
        "    unique_labels = set([label for sublist in labels for label in sublist])\n",
        "    label2id = {label: i for i, label in enumerate(unique_labels)}\n",
        "    id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "    return label2id, id2label\n"
      ],
      "metadata": {
        "id": "abGzeG17M0jJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data/train.csv')[:100]\n",
        "texts = df['title'] + ' ' + df['author'] + ' ' + df['abstract']\n",
        "label = df['Keywords']\n",
        "labels = []\n",
        "labels_bio = []\n",
        "\n",
        "for i in label:\n",
        "    tokens = word_tokenize(i)\n",
        "    for t in tokens:\n",
        "        if t != ';' and t != '.':\n",
        "            labels.append(t)\n",
        "\n",
        "labels = list(set(labels))\n",
        "\n",
        "for i in range(len(texts)):\n",
        "    tmp_l = []\n",
        "    tokens = word_tokenize(texts[i])\n",
        "\n",
        "    for t in tokens:\n",
        "        if len(tmp_l) <= max_length:\n",
        "            if t in labels:\n",
        "                tmp_l.append('B-Key')\n",
        "            else:\n",
        "                tmp_l.append('O')\n",
        "        labels_bio.append(tmp_l)\n",
        "\n",
        "# æ„å»ºæ ‡ç­¾å­—å…¸\n",
        "label2id, id2label = build_label_dict(labels_bio)\n",
        "\n",
        "# æ•°æ®é¢„å¤„ç†\n",
        "input_ids, attention_masks, encoded_labels = preprocess_data(texts, labels_bio, tokenizer, label2id, max_length=max_length)\n",
        "\n",
        "for i in range(len(input_ids)):\n",
        "    if len(input_ids[i]) != len(encoded_labels[i]):\n",
        "        print(len(input_ids[i]), len(encoded_labels[i]))\n",
        "\n",
        "# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
        "train_input_ids, test_input_ids,\\\n",
        "    train_attention_masks, test_attention_masks,\\\n",
        "    train_labels, test_labels = train_test_split(input_ids, attention_masks, encoded_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "train_input_ids = tf.convert_to_tensor(train_input_ids, dtype=tf.int32)\n",
        "train_attention_masks = tf.convert_to_tensor(train_attention_masks, dtype=tf.int32)\n",
        "train_labels = np.array(train_labels)\n",
        "train_labels = tf.convert_to_tensor(train_labels, dtype=tf.int32)\n",
        "\n",
        "test_input_ids = tf.convert_to_tensor(test_input_ids, dtype=tf.int32)\n",
        "test_attention_masks = tf.convert_to_tensor(test_attention_masks, dtype=tf.int32)\n",
        "test_labels = np.array(test_labels)\n",
        "test_labels = tf.convert_to_tensor(test_labels, dtype=tf.int32)\n",
        "\n",
        "print(train_input_ids.shape)\n",
        "print(train_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "072EBEs1dMy0",
        "outputId": "edfa6658-c486-4141-dce3-dc63eb297e02"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(80, 512)\n",
            "(80, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_G6xlQzQC73",
        "outputId": "c1486528-83b6-48aa-fe9e-3e40f6ae91c2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(80, 512), dtype=int32, numpy=\n",
              "array([[  101,  3808,  9312, ..., 13004,  3938,   102],\n",
              "       [  101,  8208,  1997, ...,     0,     0,     0],\n",
              "       [  101, 11538,  1996, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [  101,  9203,  3496, ...,     0,     0,     0],\n",
              "       [  101, 19002,  1011, ...,     0,     0,     0],\n",
              "       [  101,  2659, 11619, ...,     0,     0,     0]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### æ„å»ºæ¨¡å‹"
      ],
      "metadata": {
        "id": "u8rau2hyhDIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.backend import dtype\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# BERT + BiLSTM + CRF Model\n",
        "def build_model(bert_model, num_tags, max_length, hidden_size, lstm_units):\n",
        "    input_ids = tf.keras.layers.Input(shape=(max_length,), name='input_ids', dtype=tf.int32)\n",
        "    attention_mask = tf.keras.layers.Input(shape=(max_length,), name='attention_mask', dtype=tf.int32)\n",
        "    # token_type_ids = tf.keras.layers.Input(shape=(max_length,), name='token_type_ids', dtype=tf.int32)\n",
        "\n",
        "    # BERT layer\n",
        "    bert_output = bert_model(input_ids, attention_mask=attention_mask)[0]\n",
        "\n",
        "    # BiLSTM layer\n",
        "    bilstm_layer = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(hidden_size, return_sequences=True), merge_mode=\"sum\")\n",
        "    bilstm_layer_out = bilstm_layer(tf.keras.layers.Dropout(0.5)(bert_output))\n",
        "\n",
        "    #     lstm_layer = LSTM(hidden_size, return_sequences=True, dropout=0.5, recurrent_dropout=0.5)\n",
        "    #     lstm_layer_out = lstm_layer(bilstm_layer_out)\n",
        "\n",
        "    time_distributed_layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(num_tags))\n",
        "    time_distributed_layer_out = time_distributed_layer(bilstm_layer_out)\n",
        "\n",
        "    # CRF layer\n",
        "    crf = CRF(dtype='float32') # float32\n",
        "    crf_out = crf(tf.keras.layers.BatchNormalization()(time_distributed_layer_out))\n",
        "\n",
        "    base_model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=crf_out)\n",
        "    model = ModelWithCRFLoss(bert_model)\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-5)) # loss=tf.keras.losses.MeanSquaredError(), metrics=['accuracy']\n",
        "    # model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "# æ„å»ºæ¨¡å‹\n",
        "model = build_model(bert_model, num_tags, max_length, hidden_size, lstm_units)\n",
        "\n",
        "# è®­ç»ƒæ¨¡å‹\n",
        "csv_logger = tf.keras.callbacks.CSVLogger('bert-ner-training.log',  append=False)\n",
        "checkpointer = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"./bert-gru-bilstm-crf.tf\",\n",
        "    monitor=\"val_accuracy\",\n",
        "    verbose=0,\n",
        "    save_best_only=True)\n",
        "\n",
        "model.fit([train_input_ids, train_attention_masks], train_labels,\n",
        "          batch_size=32,\n",
        "          epochs=10,\n",
        "          callbacks=[csv_logger, checkpointer])\n",
        "\n",
        "# è¯„ä¼°æ¨¡å‹\n",
        "# evaluation = model.evaluate([test_input_ids, test_attention_masks], test_labels)\n",
        "# loss = evaluation[0]\n",
        "# accuracy = evaluation[1]\n",
        "# print(\"Loss: {:.4f}\".format(loss))\n",
        "# print(\"Accuracy: {:.4f}\".format(accuracy))\n",
        "\n",
        "# é¢„æµ‹\n",
        "predictions = model.predict([test_input_ids, test_attention_masks])"
      ],
      "metadata": {
        "id": "KdfRFjMPjUXu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "w4h19VLOZ3C1",
        "tXWKS4G0hmWD",
        "G-f-So7O3RUU",
        "VF11TCKUjUdS",
        "o8HhloKHvgMZ",
        "qzdxn5gG4bww"
      ],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}